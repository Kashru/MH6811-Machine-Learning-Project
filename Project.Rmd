---
title: "Project"
output: html_document
---

```{r}
# Read training data
train.origin <- read.table('train.csv', sep = ",", stringsAsFactors = FALSE,na.strings=c(NA,''), header=TRUE)
summary(train.origin)
sum(is.na(train.origin))

# Handle missing values in training data
library(mice)
md.pattern(train.origin)
tempData <- mice(train.origin,m=5,maxit=50,meth='pmm')
summary(tempData)
completedData <- complete(tempData)
sapply(completedData, function(x) sum(is.na(x)))
train <- completedData
library(zoo)
train <- na.locf(na.locf(train),fromLast=TRUE)
sum(is.na(train))

# Read test data
test.origin <- read.table('test.csv', sep = ",", stringsAsFactors = FALSE,na.strings=c(NA,''), header=TRUE)
summary(test.origin)
sum(is.na(test.origin))

# Handle missing values in test data
md.pattern(test.origin)
tempData2 <- mice(test.origin,m=5,maxit=50,meth='pmm')
summary(tempData2)
completedData2 <- complete(tempData2)
sapply(completedData2, function(x) sum(is.na(x)))
test <- completedData2
test <- na.locf(na.locf(test),fromLast=TRUE)
sum(is.na(test))
```


```{r}
# Divide the train and test data sets for training with a ratio of 7:3
seed <- 114514
set.seed(seed)
sub<-sample(1:nrow(train),round(nrow(train)*7/10))
length(sub)
pd.train<-train[sub,]
pd.test<-train[-sub,]
pd.train.X <- pd.train[,c(2:12)]
pd.test.X <- pd.test[,c(2:12)]
pd.train.Y <- pd.train[,13]
pd.test.Y <- pd.test[,13]
```


```{r}
# Train with knn
library(CatEncoders)
t <- function(x) {
  # check if x is numeric
  if(is.numeric(x)) {
    return (x)
  }
  l <- LabelEncoder.fit(x)
  y <- transform(l, x)
  return (y)
}
knn.train.X <- sapply(pd.train.X, t)
knn.test.X <- sapply(pd.test.X, t)
library(class)
knn.pred <- knn(train = knn.train.X, test = knn.test.X, cl = pd.train.Y, k = 5)
ErrTe=0; Kmax=50
for(k in 1:Kmax){
  knn.pred <- knn(train = knn.train.X, test = knn.test.X, cl = pd.train.Y, k = k)
  ErrTe[k]=mean(knn.pred != pd.test.Y)
}
plot(ErrTe,type="l")
print(which.min(ErrTe))
knn.pred <- knn(train = knn.train.X, test = knn.test.X, cl = pd.train.Y, k = 9)
table(knn.pred, pd.test.Y)
mean(knn.pred != pd.test.Y) #MSE
```


```{r}
#Train with Decision Tree, format selected variables as factors
library(tree)
pd.train2 <- pd.train[,c(2:13)]
pd.train2$Gender <- as.factor(pd.train2$Gender)
pd.train2$Married <- as.factor(pd.train2$Married)
pd.train2$Dependents <- as.factor(pd.train2$Dependents)
pd.train2$Education <- as.factor(pd.train2$Education)
pd.train2$Self_Employed <- as.factor(pd.train2$Self_Employed)
pd.train2$Property_Area <- as.factor(pd.train2$Property_Area)
pd.train2$Loan_Status <- as.factor(pd.train2$Loan_Status)
pd.train2$Credit_History <- as.factor(pd.train2$Credit_History)
pd.test2 <- pd.test[, c(2:13)]
pd.test2$Gender <- as.factor(pd.test2$Gender)
pd.test2$Married <- as.factor(pd.test2$Married)
pd.test2$Dependents <- as.factor(pd.test2$Dependents)
pd.test2$Education <- as.factor(pd.test2$Education)
pd.test2$Self_Employed <- as.factor(pd.test2$Self_Employed)
pd.test2$Property_Area <- as.factor(pd.test2$Property_Area)
pd.test2$Loan_Status <- as.factor(pd.test2$Loan_Status)
pd.test2$Credit_History <- as.factor(pd.test2$Credit_History)
set.seed(seed)
loan.tree <- tree(Loan_Status~.,data = pd.train2)
summary(loan.tree)
plot(loan.tree)
text(loan.tree)
loan.tree.pred <- predict(loan.tree, pd.test2, type = "class")
(loan.tree.cm = table(loan.tree.pred, pd.test2$Loan_Status))
(loan.tree.accuracy=sum(diag(loan.tree.cm))/sum(loan.tree.cm))
```


```{r}
#Applying cross-validation to determine the best tree size
set.seed(seed)
loan.tree.cv <- cv.tree(loan.tree, FUN = prune.tree) 
(loan.tree.bestsize <- max(loan.tree.cv$size[which.min(loan.tree.cv$dev)],3))
#Best decision tree confusion matrix and accuracy
loan.tree.pruned=prune.tree(loan.tree, best=loan.tree.bestsize) 
loan.tree.pruned.pred=predict(loan.tree.pruned, pd.test2, type="class")
(loan.tree.pruned.cm=table(loan.tree.pruned.pred, pd.test2$Loan_Status))
(loan.tree.pruned.accuracy=sum(diag(loan.tree.pruned.cm))/sum(loan.tree.pruned.cm))
```

```{r}
#Predict with Decision Tree selected above using real test set
test2 <- test[, c(2:12)]
test2$Gender <- as.factor(test2$Gender)
test2$Married <- as.factor(test2$Married)
test2$Dependents <- as.factor(test2$Dependents)
test2$Education <- as.factor(test2$Education)
test2$Self_Employed <- as.factor(test2$Self_Employed)
test2$Property_Area <- as.factor(test2$Property_Area)
test2$Credit_History <- as.factor(test2$Credit_History)
loan.tree.pred.final <- predict(loan.tree.pruned, test2, type = "class")
#End of Decision Tree
```

```{r}
#Train with RandomForest
library(randomForest)
set.seed(seed)
p = ncol(pd.train2)-1
loan.rf = randomForest(Loan_Status~., data = pd.train2, mtry = ceiling(sqrt(p)),
  ntree = 500, importance = TRUE)
loan.rf
#importance(loan.rf)
#varImpPlot(loan.rf)
```

```{r}
#RandomForest confusion matrix and accuracy
loan.rf.pred <- predict(loan.rf, newdata = pd.test2, type = "class")
(loan.rf.cm <- table(loan.rf.pred, pd.test2$Loan_Status))
(loan.rf.accuracy <- sum(diag(loan.rf.cm))/sum(loan.rf.cm))
```

```{r}
#Predict with RandomForest using real test data
loan.rf.pred.final <- predict(loan.rf, newdata = test2, type = "class")
#End of RandomForest
```

```{r}
#Data processing for AdaBoost
n.train <- nrow(pd.train2)
n.test <- nrow(pd.test2)
pd.train2.X=model.matrix(Loan_Status~.-1,data=pd.train2) 
pd.train2.y.ada=rep(1,n.train); pd.train2.y.ada[pd.train2$Loan_Status=="N"]=-1  
pd.test2.X=model.matrix(Loan_Status~.-1,data=pd.test2) 
pd.test2.y.ada=rep(1,n.test); pd.test2.y.ada[pd.test2$Loan_Status=="N"]=-1
```

```{r}
#Fit AdaBoost
library(JOUSBoost)
set.seed(seed)
loan.ada = adaboost(pd.train2.X, pd.train2.y.ada, n_rounds = 500)
```

```{r}
#AdaBoost confusion matrix and accuracy
loan.ada.predict = predict(loan.ada, pd.test2.X)
(loan.ada.cm = table(loan.ada.predict, pd.test2.y.ada))
(loan.ada.accuracy = sum(diag(loan.ada.cm))/sum(loan.ada.cm))
```

```{r}
#Predict with AdaBoost using real test data
n.test.final <- nrow(test2)
test2.X=model.matrix(~.-1, data=test2) 
loan.ada.predict.raw = predict(loan.ada, test2.X)
loan.ada.predict.final = rep("Y", n.test.final); loan.ada.predict.final[loan.ada.predict.raw==-1]="N"
#End of AdaBoost
```

```{r}
#Data processing for XGBoost
pd.train2.y.xgb=rep(1,n.train); pd.train2.y.xgb[pd.train2$Loan_Status=="N"]=0  
pd.test2.y.xgb=rep(1,n.test); pd.test2.y.xgb[pd.test2$Loan_Status=="N"]=0
```

```{r}
#Fit XGBoost
library(xgboost)
set.seed(seed)
loan.xgb = xgboost(data = pd.train2.X, label = pd.train2.y.xgb, nrounds = 500, verbose = FALSE)
```

```{r}
#XGBoost confusion matrix and accuracy
loan.xgb.pred.val = predict(loan.xgb, pd.test2.X)
loan.xgb.pred = rep(0, n.test); loan.xgb.pred[loan.xgb.pred.val>0.5]=1
(loan.xgb.cm = table(loan.xgb.pred, pd.test2.y.xgb))
(load.xgb.accuracy = sum(diag(loan.xgb.cm))/sum(loan.xgb.cm))
```

```{r}
#Predict with XGBoost using real test data
test2.y.xgb=rep(1,n.test.final)
loan.xgb.pred.raw = predict(loan.xgb, test2.X)
loan.xgb.pred.final = rep("N", n.test); loan.xgb.pred.final[loan.xgb.pred.raw>0.5]="Y"
#End of XGBoost
```

```{r}
# Standardization of the features:
library(CatEncoders)
t <- function(x) {
  # check if x is numeric
  if(is.numeric(x)) {
    return (x)
  }
  l <- LabelEncoder.fit(x)
  y <- transform(l, x)
  return (y)
}
ffnn.train.X <- sapply(pd.train.X, t)
ffnn.test.X <- sapply(pd.test.X, t)
ffnn.train.X.mean <- apply(ffnn.train.X, 2, mean)
ffnn.train.X.sd <- apply(ffnn.train.X, 2, sd)
ffnn.train.X.scaled<-scale(ffnn.train.X,center=ffnn.train.X.mean,scale=ffnn.train.X.sd)
ffnn.test.X.scaled<-scale(ffnn.test.X,center=ffnn.train.X.mean,scale=ffnn.train.X.sd)
ffnn.train.scaled<-data.frame(ffnn.train.X.scaled,Loan_Status=pd.train.Y)
ffnn.test.scaled<-data.frame(ffnn.test.X.scaled,Loan_Status=pd.test.Y)

ffnn.train.X.scaled<-as.matrix(ffnn.train.X.scaled)
ffnn.test.X.scaled<-as.matrix(ffnn.test.X.scaled)

```


```{r}
# Feedforward Neural Network with neuralnet
library(neuralnet)
set.seed(114514)
loan.neunet.rep <- neuralnet(Loan_Status ~ ., data = ffnn.train.scaled,
                         hidden = c(5,3), 
                         rep = 30,
                         act.fct = 'logistic',
                         err.fct = 'ce', 
                         linear.output = FALSE,
                         threshold = 0.1)
(bestrep<-which.min(loan.neunet.rep$result.matrix["error",]))
loan.neunet.rep$weights[[bestrep]]

```
```{r}
plot(loan.neunet.rep,rep=bestrep) 

```
```{r}
# Test error
loan.neunet.pred.prob <- predict(loan.neunet.rep, ffnn.test.scaled, rep=bestrep) # Predicted "Probability" for each class
loan.neunet.pred <- as.factor(max.col(loan.neunet.pred.prob))
levels(loan.neunet.pred)<-levels(as.factor(ffnn.test.scaled$Loan_Status))
pred.table <- table(loan.neunet.pred, pd.test.Y)
library("caret")
confusionMatrix(pred.table)

```

```{r}
# Real prediction on the test set
ffnn.real <- sapply(test, t)
ffnn.real.mean <- apply(ffnn.real, 2, mean)
ffnn.real.sd <- apply(ffnn.real, 2, sd)
ffnn.real.scaled <- scale(ffnn.real,center=ffnn.real.mean,scale=ffnn.real.sd)
ffnn.real.scaled <- as.matrix(ffnn.real.scaled)

loan.neunet.real.pred.prob <- predict(loan.neunet.rep, ffnn.real.scaled, rep=bestrep) # Predicted "Probability" for each class
loan.neunet.real.pred <- as.factor(max.col(loan.neunet.real.pred.prob))
levels(loan.neunet.real.pred)<-levels(as.factor(ffnn.test.scaled$Loan_Status))

summary(loan.neunet.real.pred)

```

```{r}
# Feedforward Neural Networks with Keras and TensorFlow
library(keras)
library(tensorflow)

y.keras <- as.factor(pd.train.Y)
levels(y.keras) = 0:1
y.keras.ohc = to_categorical(y.keras , num_classes = 2)

```

```{r}
set.seed(114514)
# create sequential model
ffnn.keras.model = keras_model_sequential()

# add layers, first layer needs input dimension
ffnn.keras.model %>%
  layer_dense(input_shape = ncol(ffnn.train.X.scaled), units = 10, activation = "relu") %>%
  layer_dense(units = 10, activation = "relu") %>%
  layer_dense(units = 2, activation = "softmax")

# add a loss function and optimizer
ffnn.keras.model %>%
  compile(
    loss = "categorical_crossentropy",
    optimizer = "rmsprop",
    metrics = "accuracy"
  )

# fit model with our training data set, training will be done for 200 times data set
loan.ffnn.keras = ffnn.keras.model %>%
  fit(
    x = ffnn.train.X.scaled,
    y = y.keras.ohc,
    shuffle = T,
    epochs = 100
  )

```


```{r}
library(deepviz)
library(magrittr)
ffnn.keras.model %>% plot_model()
```


```{r}
y.keras <- as.factor(pd.test.Y)
levels(y.keras) = 0:1
y.keras.test.ohc = to_categorical(y , num_classes = 2)
evaluate(ffnn.keras.model, ffnn.test.X.scaled, y.keras.test.ohc, batch_size =1)

```

```{r}
loan.keras.pred.prob<-predict(ffnn.keras.model, ffnn.test.X.scaled, y.keras.test.ohc, batch_size =1)
loan.keras.pred<-as.factor(max.col(loan.keras.pred.prob))
levels(loan.keras.pred) <- levels(as.factor(pd.test.Y))
keras.pred.table <- table(loan.keras.pred, pd.test.Y)
```


```{r}
confusionMatrix(keras.pred.table)
```

```{r}
# Real prediction on the test set
ffnn.real.scaled <- ffnn.real.scaled[,-1]
loan.keras.real.pred <- predict(ffnn.keras.model, ffnn.real.scaled)
levels(loan.keras.real.pred)<-levels(as.factor(ffnn.test.scaled$Loan_Status))

summary(loan.neunet.real.pred)

```


```{r}
# Train with Logistic Regression and regularization

# install.packages("caret",dependencies = TRUE)
library(mlbench)
library(caret)



correlationMatrix <- cor(pd.train[,7:10])
print(correlationMatrix)
# ApplicantIncome and LoanAmount are highly correlated(0.55)

library(nnet)
nn.fit1 <- multinom(Loan_Status ~ Gender + Married + Dependents + Education + Self_Employed + ApplicantIncome + CoapplicantIncome + LoanAmount+ Loan_Amount_Term + Credit_History + Property_Area,data = pd.train)
summary(nn.fit1)
nn.pred <- predict(nn.fit1, type = "class" , newdata = pd.test)
table(nn.pred,pd.test$Loan_Status)
mean(nn.pred != pd.test.Y) 
Accuracy = 1 - mean(nn.pred != pd.test.Y) ;Accuracy
importance <- varImp(nn.fit1, scale=FALSE)
print(importance)
plot(importance)

library(glmnet)

# Lasso regularization
# Loan_Status~.
library(e1071)
set.seed(1)
library(CatEncoders)
t <- function(x) {
  # check if x is numeric
  if(is.numeric(x)) {
    return (x)
  }
  l <- LabelEncoder.fit(x)
  y <- transform(l, x)
  return (y)
}

lasso.train <- sapply(pd.train, t)
lasso.test <- sapply(pd.test, t)
lasso.train = data.frame(lasso.train)
lasso.test = data.frame(lasso.test)

for(x in 1:length(lasso.train$Loan_Status)) {
  if (lasso.train$Loan_Status[x] == 2){
    lasso.train$Loan_Status[x] = 0
  }
    
}
for(x in 1:length(lasso.test$Loan_Status)) {
  if (lasso.test$Loan_Status[x] == 2){
    lasso.test$Loan_Status[x] = 0
  }
    
}
grid <- 10 ^ seq(10, -2, length = 100)
x = model.matrix(Loan_Status ~ .,lasso.train)[,-1]
lasso.cv <- cv.glmnet(x, lasso.train$Loan_Status, family = binomial, alpha = 1,lambda = grid)
plot(lasso.cv)
lasso.bestlam <- lasso.cv$lambda.min
lasso.cvmod = glmnet(x, lasso.train$Loan_Status, family = binomial, alpha = 1,lambda = lasso.bestlam)

new_x = model.matrix(Loan_Status ~ .,lasso.test)[,-1]
lasso_predict = predict(lasso.cvmod,newx = new_x)
for(x in 1:length(lasso_predict)) {
  if (lasso_predict[x] > 0.5){
    lasso_predict[x] = 1}
  if (lasso_predict[x] <= 0.5){
    lasso_predict[x] = 0}
}
table(lasso_predict,lasso.test$Loan_Status)
mean(lasso_predict != lasso.test$Loan_Status)

# we can see that credit history is much more important than the others

nn.fit2 = multinom(Loan_Status ~ Credit_History ,data = pd.train)
summary(nn.fit2)
nn.pred <- predict(nn.fit2, type = "class" , newdata = pd.test)
table(nn.pred,pd.test$Loan_Status)
mean(nn.pred != pd.test.Y)
# After deleting all the variables except credit history, we found that error rate does not change.

```


```{r}
#Train with SVM
library(e1071)
set.seed(1)
library(CatEncoders)
t <- function(x) {
  # check if x is numeric
  if(is.numeric(x)) {
    return (x)
  }
  l <- LabelEncoder.fit(x)
  y <- transform(l, x)
  return (y)
}




svm.train <- sapply(pd.train, t)
svm.test <- sapply(pd.test, t)
svm.train = data.frame(svm.train)
svm.test = data.frame(svm.test)

svm.train$Loan_Status[1]
for(x in 1:length(svm.train$Loan_Status)) {
  if (svm.train$Loan_Status[x] == 2){
    svm.train$Loan_Status[x] = 0
  }
    
}
svm.test$Loan_Status
for(x in 1:length(svm.test$Loan_Status)) {
  if (svm.test$Loan_Status[x] == 2){
    svm.test$Loan_Status[x] = 0
  }
    
}




#Linear Kernel
cost = c(0.001, 0.01, 0.1, 1,5,10,100)
tune.out <- tune(svm,Loan_Status~., 
                 data=svm.train, 
                 kernel="linear", 
                 ranges=list(cost = cost),tunecontrol=tune.control(cross=5))
summary(tune.out)
linear_svm.best <- tune.out$best.model
summary(linear_svm.best)
svm.pred <- predict(linear_svm.best, svm.test)
svm.pred  = round(svm.pred)
table(svm.pred, svm.test$Loan_Status)
mean(svm.pred != svm.test$Loan_Status )


#Polynomial Kernel
set.seed(1)
tune.out <- tune(svm,Loan_Status~., 
                 data=svm.train, 
                 kernel="polynomial", 
                 ranges=list(cost=cost,
                             degree=c(2,3,4,5)))
summary(tune.out)
poly_svm.best <- tune.out$best.model
summary(poly_svm.best)
svm.pred <- predict(poly_svm.best, svm.test)
svm.pred  = round(svm.pred)
for(x in 1:length(svm.pred)) {
  if (svm.pred[x]<1){
    svm.pred[x] = 1
  }
    
}
table(svm.pred, svm.test$Loan_Status)
mean(svm.pred != svm.test$Loan_Status )
#RBF Kernel
set.seed(1)
tune.out <- tune(svm,Loan_Status~., 
                 data=svm.train, 
                 kernel="radial", 
                 ranges=list(cost=cost,
                             gamma=c(0.01,0.1,1,10)))
summary(tune.out)
RBF_svm.best <- tune.out$best.model
summary(RBF_svm.best)
svm.pred <- predict(RBF_svm.best, svm.test)
svm.pred  = round(svm.pred)
for(x in 1:length(svm.pred)) {
  if (svm.pred[x]<1){
    svm.pred[x] = 1
  }
  
}
table(svm.pred, svm.test$Loan_Status)
mean(svm.pred != svm.test$Loan_Status )
Accuracy = 1 - mean(svm.pred != svm.test$Loan_Status );Accuracy

```
```{r}
#Utilize Logistic Regression and SVM to predict test data
logistic.pred <- predict(nn.fit2, type = "class" , newdata = test)
linear_svm.pred <- predict(linear_svm.best, type = "class" , newdata = test)
poly_svm.pred <- predict(poly_svm.best, type = "class" , newdata = test)
RBF_svm.pred <- predict(RBF_svm.best, type = "class" , newdata = test)
``