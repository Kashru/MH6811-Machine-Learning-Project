summary(test.origin)
sum(is.na(test.origin))
# Handle missing values in test data
md.pattern(test.origin)
# Handle missing values in training data
library(mice)
# Handle missing values in test data
md.pattern(test.origin)
tempData2 <- mice(test.origin,m=5,maxit=50,meth='pmm')
summary(tempData2)
completedData2 <- complete(tempData2)
sapply(completedData2, function(x) sum(is.na(x)))
test <- completedData2
# Train with knn
# Divide the train and test data sets for training with a ratio of 7:3
set.seed(114514)
sub<-sample(1:nrow(train),round(nrow(train)*7/10))
length(sub)
pd.train<-train[sub,]
pd.test<-train[-sub,]
pd.train.X <- pd.train[,-"Loan_Status"]
pd.train.X <- pd.train[,-c("Loan_Status")]
View(pd.train)
pd.train.X <- pd.train["Gender":"Property_Area"]
pd.train.X <- pd.train[,c("Gender":"Property_Area")]
pd.train.X <- pd.train[,c('Gender':'Property_Area')]
pd.train.X <- pd.train[,c(2,3,4,5,6,7,8,9,10,11,12)]
pd.train.X <- pd.train[,c(2:12)]
View(pd.train.X)
pd.test.X <- pd.test[,c(2:12)]
pd.test.X <- pd.test[,c(2:12)]
pd.train.Y <- pd.train[13]
View(pd.train.Y)
pd.test.Y <- pd.test[13]
knn.pred <- knn(train = pd.train.X, test = pd.test.X, cl = pd.train.Y, k = 5)
library(class)
knn.pred <- knn(train = pd.train.X, test = pd.test.X, cl = pd.train.Y, k = 5)
train %>% fill(Gender)
fill(train)
library(tidyr)
fill(train)
fill(test)
# Train with knn
# Divide the train and test data sets for training with a ratio of 7:3
set.seed(114514)
sub<-sample(1:nrow(train),round(nrow(train)*7/10))
length(sub)
pd.train<-train[sub,]
pd.test<-train[-sub,]
pd.train.X <- pd.train[,c(2:12)]
pd.test.X <- pd.test[,c(2:12)]
pd.train.Y <- pd.train[13]
pd.test.Y <- pd.test[13]
library(class)
knn.pred <- knn(train = pd.train.X, test = pd.test.X, cl = pd.train.Y, k = 5)
View(train)
# Read training data
train.origin <- read.table('train.csv', sep = ",", stringsAsFactors = FALSE,na.strings=c(NA,''), header=TRUE)
summary(train.origin)
sum(is.na(train.origin))
# Handle missing values in training data
library(mice)
md.pattern(train.origin)
tempData <- mice(train.origin,m=5,maxit=50,meth='pmm')
summary(tempData)
completedData <- complete(tempData)
sapply(completedData, function(x) sum(is.na(x)))
train <- completedData
library(tidyr)
fill(train)
sum(is.na(train))
fill.NAs(train)
train %>% fill(Gender, Married, Dependents, Self_Employed)
sum(is.na(train))
View(train)
library(zoo)
install.packages("zoo")
library(zoo)
na.locf(na.locf(train),fromLast=TRUE)
sum(is.na(train))
train <-na.locf(na.locf(train),fromLast=TRUE)
sum(is.na(train))
# Read training data
train.origin <- read.table('train.csv', sep = ",", stringsAsFactors = FALSE,na.strings=c(NA,''), header=TRUE)
summary(train.origin)
sum(is.na(train.origin))
# Handle missing values in training data
library(mice)
md.pattern(train.origin)
tempData <- mice(train.origin,m=5,maxit=50,meth='pmm')
summary(tempData)
completedData <- complete(tempData)
sapply(completedData, function(x) sum(is.na(x)))
train <- completedData
library(tidyr)
train <- fill(train)
sum(is.na(train))
library(zoo)
train <- na.locf(na.locf(df1),fromLast=TRUE)
train <- na.locf(na.locf(train),fromLast=TRUE)
sum(is.na(train))
# Read test data
test.origin <- read.table('test.csv', sep = ",", stringsAsFactors = FALSE,na.strings=c(NA,''), header=TRUE)
summary(test.origin)
sum(is.na(test.origin))
# Handle missing values in test data
md.pattern(test.origin)
tempData2 <- mice(test.origin,m=5,maxit=50,meth='pmm')
summary(tempData2)
completedData2 <- complete(tempData2)
sapply(completedData2, function(x) sum(is.na(x)))
test <- completedData2
test <- na.locf(na.locf(test),fromLast=TRUE)
sum(is.na(test))
# Train with knn
# Divide the train and test data sets for training with a ratio of 7:3
set.seed(114514)
sub<-sample(1:nrow(train),round(nrow(train)*7/10))
length(sub)
pd.train<-train[sub,]
pd.test<-train[-sub,]
pd.train.X <- pd.train[,c(2:12)]
pd.test.X <- pd.test[,c(2:12)]
pd.train.Y <- pd.train[13]
pd.test.Y <- pd.test[13]
library(class)
knn.pred <- knn(train = pd.train.X, test = pd.test.X, cl = pd.train.Y, k = 5)
pd.train.Y <- pd.train[,13]
pd.test.Y <- pd.test[,13]
library(class)
knn.pred <- knn(train = pd.train.X, test = pd.test.X, cl = pd.train.Y, k = 5)
table(knn.pred, pd.test.Y)
pd.train.Y <- pd.train[,1]
sum(is.na(pd.train.X))
sum(is.na(pd.train.Y))
sum(is.na(pd.test.X))
pd.train.Y <- pd.train[,13]
sum(is.na(pd.train.Y))
library(class)
knn.pred <- knn(train = pd.train.X, test = pd.test.X, cl = pd.train.Y, k = 5)
View(pd.train.X)
sum(is.na(pd.train.X))
knn.pred <- knn(train = pd.train.X, test = pd.test.X, cl = pd.train.Y, k = 5)
View(pd.train.X)
View(pd.train.X)
pd.train.X <- unclass(pd.train.X)
pd.test.X <- unclass(pd.test.X)
knn.pred <- knn(train = pd.train.X, test = pd.test.X, cl = pd.train.Y, k = 5)
pd.train.Y <- unclass(pd.train.Y)
pd.test.Y <- unclass(pd.test.Y)
library(class)
knn.pred <- knn(train = pd.train.X, test = pd.test.X, cl = pd.train.Y, k = 5)
# Train with knn
# Divide the train and test data sets for training with a ratio of 7:3
set.seed(114514)
sub<-sample(1:nrow(train),round(nrow(train)*7/10))
length(sub)
pd.train<-train[sub,]
pd.test<-train[-sub,]
pd.train.X <- pd.train[,c(2:12)]
pd.test.X <- pd.test[,c(2:12)]
pd.train.Y <- pd.train[,13]
pd.test.Y <- pd.test[,13]
pd.train.X <- sapply(pd.train[,c(2:12)],unclass)
View(pd.train.X)
pd.test.X <- sapply(pd.test[,c(2:12)],unclass)
pd.train.Y <- pd.train[,13]
pd.test.Y <- pd.test[,13]
library(class)
knn.pred <- knn(train = pd.train.X, test = pd.test.X, cl = pd.train.Y, k = 5)
pd.train.Y <- unclass(pd.train[,13])
pd.test.Y <- unclass(pd.test[,13])
library(class)
knn.pred <- knn(train = pd.train.X, test = pd.test.X, cl = pd.train.Y, k = 5)
table(knn.pred, pd.test.Y)
# Train with knn
# Divide the train and test data sets for training with a ratio of 7:3
set.seed(114514)
sub<-sample(1:nrow(train),round(nrow(train)*7/10))
length(sub)
pd.train<-train[sub,]
pd.test<-train[-sub,]
pd.train.X <- pd.train[,c(2:12)]
pd.test.X <- pd.test[,c(2:12)]
pd.train.Y <- pd.train[,13]
pd.test.Y <- pd.test[,13]
t <- function(x) {
# check if x is numeric
if(is.numeric(x)) {
return (x)
}
l <- LabelEncoder.fit(x)
y <- transform(l, x)
return (y)
}
knn.train.X <- sapply(pd.train.X, t)
knn.test.X <- sapply(pd.train.Y, t)
library(CatEncoders)
install.packages("CatEncoders")
library(CatEncoders)
knn.train.X <- sapply(pd.train.X, t)
knn.test.X <- sapply(pd.train.Y, t)
knn.pred <- knn(train = knn.train.X, test = knn.test.X, cl = pd.train.Y, k = 5)
knn.test.X <- sapply(pd.test.X, t)
knn.pred <- knn(train = knn.train.X, test = knn.test.X, cl = pd.train.Y, k = 5)
table(knn.pred, pd.test.Y)
# Read training data
train.origin <- read.table('train.csv', sep = ",", stringsAsFactors = FALSE,na.strings=c(NA,''), header=TRUE)
summary(train.origin)
sum(is.na(train.origin))
# Handle missing values in training data
library(mice)
md.pattern(train.origin)
tempData <- mice(train.origin,m=5,maxit=50,meth='pmm')
summary(tempData)
completedData <- complete(tempData)
sapply(completedData, function(x) sum(is.na(x)))
train <- completedData
library(zoo)
train <- na.locf(na.locf(train),fromLast=TRUE)
sum(is.na(train))
# Read test data
test.origin <- read.table('test.csv', sep = ",", stringsAsFactors = FALSE,na.strings=c(NA,''), header=TRUE)
summary(test.origin)
sum(is.na(test.origin))
# Handle missing values in test data
md.pattern(test.origin)
tempData2 <- mice(test.origin,m=5,maxit=50,meth='pmm')
summary(tempData2)
completedData2 <- complete(tempData2)
sapply(completedData2, function(x) sum(is.na(x)))
test <- completedData2
test <- na.locf(na.locf(test),fromLast=TRUE)
sum(is.na(test))
# Train with knn
# Divide the train and test data sets for training with a ratio of 7:3
set.seed(114514)
sub<-sample(1:nrow(train),round(nrow(train)*7/10))
length(sub)
pd.train<-train[sub,]
pd.test<-train[-sub,]
pd.train.X <- pd.train[,c(2:12)]
pd.test.X <- pd.test[,c(2:12)]
pd.train.Y <- pd.train[,13]
pd.test.Y <- pd.test[,13]
library(CatEncoders)
t <- function(x) {
# check if x is numeric
if(is.numeric(x)) {
return (x)
}
l <- LabelEncoder.fit(x)
y <- transform(l, x)
return (y)
}
knn.train.X <- sapply(pd.train.X, t)
knn.test.X <- sapply(pd.test.X, t)
library(class)
knn.pred <- knn(train = knn.train.X, test = knn.test.X, cl = pd.train.Y, k = 5)
table(knn.pred, pd.test.Y)
mean(knn.pred != pd.test.Y) #MSE
# Standardization of the features:
pd.train.X.mean <- apply(pd.train.X, 2, mean)
pd.train.X.sd <- apply(pd.train.X, 2, sd)
pd.train.X.scaled<-scale(pd.train.X,center=pd.train.X.mean,scale=pd.train.X.sd)
# Standardization of the features:
library(CatEncoders)
t <- function(x) {
# check if x is numeric
if(is.numeric(x)) {
return (x)
}
l <- LabelEncoder.fit(x)
y <- transform(l, x)
return (y)
}
ffnn.train.X <- sapply(pd.train.X, t)
ffnn.test.X <- sapply(pd.test.X, t)
ffnn.train.X.mean <- apply(ffnn.train.X, 2, mean)
ffnn.train.X.sd <- apply(ffnn.train.X, 2, sd)
ffnn.train.X.scaled<-scale(ffnn.train.X,center=ffnn.train.X.mean,scale=ffnn.train.X.sd)
ffnn.test.X.scaled<-scale(ffnn.test.X,center=ffnn.train.X.mean,scale=ffnn.train.X.sd)
# Standardization of the features:
library(CatEncoders)
t <- function(x) {
# check if x is numeric
if(is.numeric(x)) {
return (x)
}
l <- LabelEncoder.fit(x)
y <- transform(l, x)
return (y)
}
ffnn.train.X <- sapply(pd.train.X, t)
ffnn.test.X <- sapply(pd.test.X, t)
ffnn.train.X.mean <- apply(ffnn.train.X, 2, mean)
ffnn.train.X.sd <- apply(ffnn.train.X, 2, sd)
ffnn.train.X.scaled<-scale(ffnn.train.X,center=ffnn.train.X.mean,scale=ffnn.train.X.sd)
ffnn.test.X.scaled<-scale(ffnn.test.X,center=ffnn.train.X.mean,scale=ffnn.train.X.sd)
ffnn.train.scaled<-data.frame(ffnn.train.X.scaled,Loan_Status=pd.train.Y)
ffnn.test.scaled<-data.frame(ffnn.test.X.scaled,Loan_Status=pd.test.Y)
# Feedforward Neural Network with neuralnet
library(neuralnet)
set.seed(114514)
loan.neunet.rep <- neuralnet(Loan_Status ~ ., data = ffnn.train.scaled,
hidden = c(5,3),
rep = 30,
act.fct = 'logistic',
err.fct = 'ce',
linear.output = FALSE)
devtools::install_github("bips-hb/neuralnet")
detach("package:neuralnet", unload = TRUE)
devtools::install_github("bips-hb/neuralnet")
# Feedforward Neural Network with neuralnet
library(neuralnet)
set.seed(114514)
loan.neunet.rep <- neuralnet(Loan_Status ~ ., data = ffnn.train.scaled,
hidden = c(5,3),
rep = 30,
act.fct = 'logistic',
err.fct = 'ce',
linear.output = FALSE)
(bestrep<-which.min(loan.neunet.rep$result.matrix["error",]))
laon.neunet.rep$weights[[bestrep]]
# Feedforward Neural Network with neuralnet
library(neuralnet)
set.seed(114514)
loan.neunet.rep <- neuralnet(Loan_Status ~ ., data = ffnn.train.scaled,
hidden = c(5,3),
rep = 30,
act.fct = 'logistic',
err.fct = 'ce',
linear.output = FALSE)
(bestrep<-which.min(loan.neunet.rep$result.matrix["error",]))
loan.neunet.rep$weights[[bestrep]]
# Feedforward Neural Network with neuralnet
library(neuralnet)
set.seed(114514)
loan.neunet.rep <- neuralnet(Loan_Status ~ ., data = ffnn.train.scaled,
hidden = c(5,3),
rep = 100,
act.fct = 'logistic',
err.fct = 'ce',
linear.output = FALSE)
(bestrep<-which.min(loan.neunet.rep$result.matrix["error",]))
loan.neunet.rep$weights[[bestrep]]
loan.neunet.rep$weights[bestrep]
loan.neunet.rep$weights[[bestrep]]
plot(loan.neunet.rep,rep=bestrep)
# Standardization of the features:
library(CatEncoders)
t <- function(x) {
# check if x is numeric
if(is.numeric(x)) {
return (x)
}
l <- LabelEncoder.fit(x)
y <- transform(l, x)
return (y)
}
ffnn.train.X <- sapply(pd.train.X, t)
ffnn.test.X <- sapply(pd.test.X, t)
ffnn.train.X.mean <- apply(ffnn.train.X, 2, mean)
ffnn.train.X.sd <- apply(ffnn.train.X, 2, sd)
ffnn.train.X.scaled<-scale(ffnn.train.X,center=ffnn.train.X.mean,scale=ffnn.train.X.sd)
ffnn.test.X.scaled<-scale(ffnn.test.X,center=ffnn.train.X.mean,scale=ffnn.train.X.sd)
ffnn.train.scaled<-data.frame(ffnn.train.X.scaled,Loan_Status=pd.train.Y)
ffnn.test.scaled<-data.frame(ffnn.test.X.scaled,Loan_Status=pd.test.Y)
ffnn.train.X.scaled<-as.matrix(ffnn.train.X.scaled)
ffnn.test.X.scaled<-as.matrix(ffnn.test.X.scaled)
# Feedforward Neural Network with neuralnet
library(neuralnet)
set.seed(114514)
loan.neunet.rep <- neuralnet(Loan_Status ~ ., data = ffnn.train.scaled,
hidden = c(5,3),
rep = 30,
act.fct = 'logistic',
err.fct = 'ce',
linear.output = FALSE)
View(ffnn.train.X.scaled)
(bestrep<-which.min(loan.neunet.rep$result.matrix["error",]))
loan.neunet.rep$weights[[bestrep]]
# Feedforward Neural Network (Classification) with nnet
set.seed(114514)
loan.nn.rep <- nn.rep(rep = 30, Loan_Status ~ ., data = ffnn.train.scaled,
size = 2, trace = FALSE)
# Feedforward Neural Network (Classification) with nnet
set.seed(114514)
library(nnet)
loan.nn.rep <- nn.rep(rep = 30, Loan_Status ~ ., data = ffnn.train.scaled,
size = 2, trace = FALSE)
# Feedforward Neural Network (Classification) with nnet
set.seed(114514)
library(nnet)
nn.rep <- function(rep, ...) {
v.min <- Inf                 # initialize v.min
for (r in 1:rep) {           # repeat nnet
nn.temp <- nnet(...)      # fit the first nnet
v.temp <- nn.temp$value   # store the cost
if (v.temp < v.min) {     # choose better weights
v.min <- v.temp
nn.min <- nn.temp
}
}
return(nn.min)
}
loan.nn.rep <- nn.rep(rep = 30, Loan_Status ~ ., data = ffnn.train.scaled,
size = 2, trace = FALSE)
View(ffnn.train.X.scaled)
# Feedforward Neural Network with neuralnet
library(neuralnet)
set.seed(114514)
loan.neunet.rep <- neuralnet(Loan_Status ~ ., data = ffnn.train.scaled,
hidden = c(5,3),
rep = 30,
act.fct = 'logistic',
err.fct = 'ce',
linear.output = FALSE,
threshold = 0.1)
(bestrep<-which.min(loan.neunet.rep$result.matrix["error",]))
loan.neunet.rep$weights[[bestrep]]
plot(loan.neunet.rep,rep=bestrep)
# Test error
loan.neunet.pred.prob <- predict(loan.neunet.rep, ffnn.test.scaled, rep=bestrep) # Predicted "Probability" for each class
loan.neunet.pred <- as.factor(max.col(loan.neunet.pred.prob))
levels(loan.neunet.pred)<-levels(ffnn.train.scaled$Loan_Status)
# Test error
loan.neunet.pred.prob <- predict(loan.neunet.rep, ffnn.test.scaled, rep=bestrep) # Predicted "Probability" for each class
loan.neunet.pred <- as.factor(max.col(loan.neunet.pred.prob))
table(loan.neunet.pred, pd.test.Y)
# Test error
loan.neunet.pred.prob <- predict(loan.neunet.rep, ffnn.test.scaled, rep=bestrep) # Predicted "Probability" for each class
loan.neunet.pred <- as.factor(max.col(loan.neunet.pred.prob))
loan.neunet.pred <- c("N","Y")[apply(loan.neunet.pred,1,which.max)]
# Test error
loan.neunet.pred.prob <- predict(loan.neunet.rep, ffnn.test.scaled, rep=bestrep) # Predicted "Probability" for each class
loan.neunet.pred <- as.factor(max.col(loan.neunet.pred.prob))
table(loan.neunet.pred, pd.test.Y)
confusionMatrix(table)
# Test error
loan.neunet.pred.prob <- predict(loan.neunet.rep, ffnn.test.scaled, rep=bestrep) # Predicted "Probability" for each class
loan.neunet.pred <- as.factor(max.col(loan.neunet.pred.prob))
table(loan.neunet.pred, pd.test.Y)
library("caret")
install.packages("caret")
# Test error
loan.neunet.pred.prob <- predict(loan.neunet.rep, ffnn.test.scaled, rep=bestrep) # Predicted "Probability" for each class
loan.neunet.pred <- as.factor(max.col(loan.neunet.pred.prob))
table(loan.neunet.pred, pd.test.Y)
library("caret")
confusionMatrix(table)
# Test error
loan.neunet.pred.prob <- predict(loan.neunet.rep, ffnn.test.scaled, rep=bestrep) # Predicted "Probability" for each class
loan.neunet.pred <- as.factor(max.col(loan.neunet.pred.prob))
pred.table <- table(loan.neunet.pred, pd.test.Y)
library("caret")
confusionMatrix(pred.table)
# Test error
loan.neunet.pred.prob <- predict(loan.neunet.rep, ffnn.test.scaled, rep=bestrep) # Predicted "Probability" for each class
loan.neunet.pred <- as.factor(max.col(loan.neunet.pred.prob))
levels(loan.neunet.pred)<-levels(pd.test.Y)
# Test error
loan.neunet.pred.prob <- predict(loan.neunet.rep, ffnn.test.scaled, rep=bestrep) # Predicted "Probability" for each class
loan.neunet.pred <- as.factor(max.col(loan.neunet.pred.prob))
levels(loan.neunet.pred)<-levels(pd.test.Y)
# Test error
loan.neunet.pred.prob <- predict(loan.neunet.rep, ffnn.test.scaled, rep=bestrep) # Predicted "Probability" for each class
loan.neunet.pred <- as.factor(max.col(loan.neunet.pred.prob))
levels(loan.neunet.pred)<-levels(pd.test.Y)
# Test error
loan.neunet.pred.prob <- predict(loan.neunet.rep, ffnn.test.scaled, rep=bestrep) # Predicted "Probability" for each class
loan.neunet.pred <- as.factor(max.col(loan.neunet.pred.prob))
levels(loan.neunet.pred)<-levels(pd.test.Y)
# Test error
loan.neunet.pred.prob <- predict(loan.neunet.rep, ffnn.test.scaled, rep=bestrep) # Predicted "Probability" for each class
loan.neunet.pred <- as.factor(max.col(loan.neunet.pred.prob))
levels(loan.neunet.pred)<-levels(pd.test.Y)
# Test error
loan.neunet.pred.prob <- predict(loan.neunet.rep, ffnn.test.scaled, rep=bestrep) # Predicted "Probability" for each class
loan.neunet.pred <- as.factor(max.col(loan.neunet.pred.prob))
levels(loan.neunet.pred)<-levels(ffnn.test.scaled$Loan_Status)
wtf <- ffnn.test.scaled$Loan_Status
# Standardization of the features:
library(CatEncoders)
t <- function(x) {
# check if x is numeric
if(is.numeric(x)) {
return (x)
}
l <- LabelEncoder.fit(x)
y <- transform(l, x)
return (y)
}
ffnn.train.X <- sapply(pd.train.X, t)
ffnn.test.X <- sapply(pd.test.X, t)
ffnn.train.X.mean <- apply(ffnn.train.X, 2, mean)
ffnn.train.X.sd <- apply(ffnn.train.X, 2, sd)
ffnn.train.X.scaled<-scale(ffnn.train.X,center=ffnn.train.X.mean,scale=ffnn.train.X.sd)
ffnn.test.X.scaled<-scale(ffnn.test.X,center=ffnn.train.X.mean,scale=ffnn.train.X.sd)
ffnn.train.scaled<-data.frame(ffnn.train.X.scaled,Loan_Status=pd.train.Y)
ffnn.test.scaled<-data.frame(ffnn.test.X.scaled,Loan_Status=pd.test.Y)
ffnn.train.X.scaled<-as.matrix(ffnn.train.X.scaled)
ffnn.test.X.scaled<-as.matrix(ffnn.test.X.scaled)
wtf <- ffnn.test.scaled$Loan_Status
wtf <- as.factor(ffnn.test.scaled$Loan_Status)
# Test error
loan.neunet.pred.prob <- predict(loan.neunet.rep, ffnn.test.scaled, rep=bestrep) # Predicted "Probability" for each class
loan.neunet.pred <- as.factor(max.col(loan.neunet.pred.prob))
levels(loan.neunet.pred)<-levels(as.factor(ffnn.test.scaled$Loan_Status))
pred.table <- table(loan.neunet.pred, pd.test.Y)
library("caret")
confusionMatrix(pred.table)
