<<<<<<< Updated upstream
# create sequential model
ffnn.keras.model = keras_model_sequential()
# add layers, first layer needs input dimension
ffnn.keras.model %>%
layer_dense(input_shape = ncol(ffnn.train.X.scaled), units = 10, activation = "relu") %>%
layer_dense(units = 10, activation = "relu") %>%
layer_dense(units = 3, activation = "softmax")
# add a loss function and optimizer
ffnn.keras.model %>%
compile(
loss = "categorical_crossentropy",
optimizer = "rmsprop",
metrics = "accuracy"
)
# fit model with our training data set, training will be done for 200 times data set
loan.ffnn.keras = ffnn.keras.model %>%
fit(
x = ffnn.train.X.scaled,
y = y.keras.ohc,
shuffle = T,
epochs = 300
)
set.seed(114514)
# create sequential model
ffnn.keras.model = keras_model_sequential()
# add layers, first layer needs input dimension
ffnn.keras.model %>%
layer_dense(input_shape = ncol(ffnn.train.X.scaled), units = 10, activation = "relu") %>%
layer_dense(units = 10, activation = "relu") %>%
layer_dense(units = 3, activation = "softmax")
# add a loss function and optimizer
ffnn.keras.model %>%
compile(
loss = "categorical_crossentropy",
optimizer = "rmsprop",
metrics = "accuracy"
)
# fit model with our training data set, training will be done for 200 times data set
loan.ffnn.keras = ffnn.keras.model %>%
fit(
x = ffnn.train.X.scaled,
y = as.factor(pd.train.Y),
shuffle = T,
epochs = 300
)
set.seed(114514)
# create sequential model
ffnn.keras.model = keras_model_sequential()
# add layers, first layer needs input dimension
ffnn.keras.model %>%
layer_dense(input_shape = ncol(ffnn.train.X.scaled), units = 10, activation = "relu") %>%
layer_dense(units = 10, activation = "relu") %>%
layer_dense(units = 3, activation = "softmax")
# add a loss function and optimizer
ffnn.keras.model %>%
compile(
loss = "categorical_crossentropy",
optimizer = "rmsprop",
metrics = "accuracy"
)
# fit model with our training data set, training will be done for 200 times data set
loan.ffnn.keras = ffnn.keras.model %>%
fit(
x = ffnn.train.X.scaled,
y = y.keras.ohc,
shuffle = T,
epochs = 30
)
View(ffnn.train.X.scaled)
# Standardization of the features:
library(CatEncoders)
t <- function(x) {
# check if x is numeric
if(is.numeric(x)) {
return (x)
}
l <- LabelEncoder.fit(x)
y <- transform(l, x)
return (y)
}
ffnn.train.X <- sapply(pd.train.X, t)
ffnn.test.X <- sapply(pd.test.X, t)
ffnn.train.X.mean <- apply(ffnn.train.X, 2, mean)
ffnn.train.X.sd <- apply(ffnn.train.X, 2, sd)
ffnn.train.X.scaled<-scale(ffnn.train.X,center=ffnn.train.X.mean,scale=ffnn.train.X.sd)
ffnn.test.X.scaled<-scale(ffnn.test.X,center=ffnn.train.X.mean,scale=ffnn.train.X.sd)
ffnn.train.scaled<-data.frame(ffnn.train.X.scaled,Loan_Status=pd.train.Y)
ffnn.test.scaled<-data.frame(ffnn.test.X.scaled,Loan_Status=pd.test.Y)
ffnn.train.X.scaled<-as.matrix(ffnn.train.X.scaled)
ffnn.test.X.scaled<-as.matrix(ffnn.test.X.scaled)
set.seed(114514)
# create sequential model
ffnn.keras.model = keras_model_sequential()
# add layers, first layer needs input dimension
ffnn.keras.model %>%
layer_dense(input_shape = ncol(ffnn.train.X.scaled), units = 10, activation = "relu") %>%
layer_dense(units = 10, activation = "relu") %>%
layer_dense(units = 3, activation = "softmax")
# add a loss function and optimizer
ffnn.keras.model %>%
compile(
loss = "categorical_crossentropy",
optimizer = "rmsprop",
metrics = "accuracy"
)
# fit model with our training data set, training will be done for 200 times data set
loan.ffnn.keras = ffnn.keras.model %>%
fit(
x = ffnn.train.X.scaled,
y = y.keras.ohc,
shuffle = T,
epochs = 30
)
set.seed(114514)
# create sequential model
ffnn.keras.model = keras_model_sequential()
# add layers, first layer needs input dimension
ffnn.keras.model %>%
layer_dense(input_shape = ncol(ffnn.train.X.scaled), units = 10, activation = "relu") %>%
layer_dense(units = 10, activation = "relu") %>%
layer_dense(units = 3, activation = "softmax")
# add a loss function and optimizer
ffnn.keras.model %>%
compile(
loss = "categorical_crossentropy",
optimizer = "rmsprop",
metrics = "accuracy"
)
# fit model with our training data set, training will be done for 200 times data set
loan.ffnn.keras = ffnn.keras.model %>%
fit(
x = ffnn.train.X.scaled,
y = y.keras.ohc,
shuffle = T,
epochs = 30
)
set.seed(114514)
# create sequential model
ffnn.keras.model = keras_model_sequential()
# add layers, first layer needs input dimension
ffnn.keras.model %>%
layer_dense(input_shape = ncol(ffnn.train.X.scaled), units = 10, activation = "relu") %>%
layer_dense(units = 10, activation = "relu") %>%
layer_dense(units = 3, activation = "softmax")
# add a loss function and optimizer
ffnn.keras.model %>%
compile(
loss = "categorical_crossentropy",
optimizer = "rmsprop",
metrics = "accuracy"
)
# fit model with our training data set, training will be done for 200 times data set
loan.ffnn.keras = ffnn.keras.model %>%
fit(
x = as.matrix(ffnn.train.X.scaled),
y = y.keras.ohc,
shuffle = T,
epochs = 30
)
# Standardization of the features:
=======
nn.fit <- multinom(Loan_Status~x,data = pd.train)
nn.pred <- predict(nn.fit, type = "class" , newdata = pd.test)
print(mean(knn.pred != pd.test.Y))
}
nn.fit <- multinom(Loan_Status~Married,data = pd.train)
nn.pred <- predict(nn.fit, type = "class" , newdata = pd.test)
print(mean(knn.pred != pd.test.Y))
for(x in Independent_variable){
nn.fit <- multinom(Loan_Status~x,data = pd.train)
nn.pred <- predict(nn.fit, type = "class" , newdata = pd.test)
print(mean(knn.pred != pd.test.Y))
}
nn.fit <- multinom(Loan_Status~Married,data = pd.train)
nn.pred <- predict(nn.fit, type = "class" , newdata = pd.test)
print(mean(knn.pred != pd.test.Y))
nn.fit <- multinom(Loan_Status~Dependents,data = pd.train)
nn.pred <- predict(nn.fit, type = "class" , newdata = pd.test)
print(mean(knn.pred != pd.test.Y))
nn.fit <- multinom(Loan_Status~Education,data = pd.train)
nn.pred <- predict(nn.fit, type = "class" , newdata = pd.test)
print(mean(knn.pred != pd.test.Y))
nn.fit <- multinom(Loan_Status~Education,data = pd.train)
summary(nn.fit)
x = c("Gender", "Married", "Dependents", "Education", "Self_Employed", "ApplicantIncome", "CoapplicantIncome", "LoanAmount", "Loan_Amount_Term", "Credit_History", "Property_Area")
ErrTe=0
for(i in 1:11){
nn.fit <- multinom(Loan_Status~x[i],data = pd.train)
nn.pred <- predict(nn.fit, type = "class" , newdata = pd.test)
print(mean(knn.pred != pd.test.Y))
}
for(i in 1:11){
print(x[i])
}
nn.fit <- multinom(Loan_Status~"Gender",data = pd.train)
nn.fit <- multinom(Loan_Status~Gender,data = pd.train)
nn.pred <- predict(nn.fit, type = "class" , newdata = pd.test)
print(mean(knn.pred != pd.test.Y))
nn.fit <- multinom(Loan_Status~x[1],data = pd.train)
nn.fit <- multinom(Loan_Status~as.character(x[1]),data = pd.train)
nn.fit = multinom(Loan_Status ~ Married + Dependents + Education + Self_Employed + ApplicantIncome)
x = c("Gender", "Married", "Dependents", "Education", "Self_Employed", "ApplicantIncome", "CoapplicantIncome", "LoanAmount", "Loan_Amount_Term", "Credit_History", "Property_Area")
nn.fit = multinom(Loan_Status ~ Married + Dependents + Education + Self_Employed + ApplicantIncome,data = pd.train)
nn.pred <- predict(nn.fit, type = "class" , newdata = pd.test)
mean(knn.pred != pd.test.Y)
summary(nn.fit)
nn.pred <- predict(nn.fit, type = "class" , newdata = pd.test)
mean(knn.pred != pd.test.Y)
nn.fit <- multinom(Loan_Status ~ Married + Dependents + Education + Self_Employed + ApplicantIncome + CoapplicantIncome + LoanAmount+ Loan_Amount_Term + Credit_History + Property_Area,data = pd.train)
summary(nn.fit)
nn.prob <- predict(nn.fit, type = "probs" , newdata = pd.test)
nn.pred <- predict(nn.fit, type = "class" , newdata = pd.test)
CM = table(nn.pred,pd.test$Loan_Status)
mean(knn.pred != pd.test.Y)
nn.fit = multinom(Loan_Status ~ Married + Dependents + Education + Self_Employed + ApplicantIncome,data = pd.train)
summary(nn.fit)
nn.pred <- predict(nn.fit, type = "class" , newdata = pd.test)
mean(knn.pred != pd.test.Y)
table(nn.pred,pd.test$Loan_Status)
nn.fit <- multinom(Loan_Status ~ Married + Dependents + Education + Self_Employed + ApplicantIncome + CoapplicantIncome + LoanAmount+ Loan_Amount_Term + Credit_History + Property_Area,data = pd.train)
summary(nn.fit)
nn.prob <- predict(nn.fit, type = "probs" , newdata = pd.test)
nn.pred <- predict(nn.fit, type = "class" , newdata = pd.test)
table(nn.pred,pd.test$Loan_Status)
mean(knn.pred != pd.test.Y)
mean(nn.pred != pd.test.Y)
x = c("Gender", "Married", "Dependents", "Education", "Self_Employed", "ApplicantIncome", "CoapplicantIncome", "LoanAmount", "Loan_Amount_Term", "Credit_History", "Property_Area")
nn.fit = multinom(Loan_Status ~ Married + Dependents + Education + Self_Employed + ApplicantIncome,data = pd.train)
summary(nn.fit)
nn.pred <- predict(nn.fit, type = "class" , newdata = pd.test)
mean(knn.pred != pd.test.Y)
table(nn.pred,pd.test$Loan_Status)
x = c("Gender", "Married", "Dependents", "Education", "Self_Employed", "ApplicantIncome", "CoapplicantIncome", "LoanAmount", "Loan_Amount_Term", "Credit_History", "Property_Area")
nn.fit = multinom(Loan_Status ~ Married + Dependents + Education + Self_Employed + ApplicantIncome,data = pd.train)
summary(nn.fit)
nn.pred <- predict(nn.fit, type = "class" , newdata = pd.test)
mean(nn.pred != pd.test.Y)
table(nn.pred,pd.test$Loan_Status)
nn.fit <- multinom(Loan_Status ~ Married + Dependents + Education + Self_Employed + ApplicantIncome + CoapplicantIncome + LoanAmount+ Loan_Amount_Term + Credit_History + Property_Area,data = pd.train)
summary(nn.fit)
nn.prob <- predict(nn.fit, type = "probs" , newdata = pd.test)
nn.pred <- predict(nn.fit, type = "class" , newdata = pd.test)
table(nn.pred,pd.test$Loan_Status)
mean(nn.pred != pd.test.Y)
nn.fit = multinom(Loan_Status ~ Married + Dependents + Education + Self_Employed + ApplicantIncome,data = pd.train)
summary(nn.fit)
nn.pred <- predict(nn.fit, type = "class" , newdata = pd.test)
mean(nn.pred != pd.test.Y)
table(nn.pred,pd.test$Loan_Status)
x = c(Gender)
nn.fit = multinom(Loan_Status ~ Married + Dependents + Education + Self_Employed + ApplicantIncome + CoapplicantIncome + LoanAmount+ Loan_Amount_Term + Credit_History,data = pd.train)
summary(nn.fit)
nn.pred <- predict(nn.fit, type = "class" , newdata = pd.test)
mean(nn.pred != pd.test.Y)
table(nn.pred,pd.test$Loan_Status)
nn.fit1 <- multinom(Loan_Status ~ Married + Dependents + Education + Self_Employed + ApplicantIncome + CoapplicantIncome + LoanAmount+ Loan_Amount_Term + Credit_History + Property_Area,data = pd.train)
summary(nn.fit)
nn.prob <- predict(nn.fit1, type = "probs" , newdata = pd.test)
nn.pred <- predict(nn.fit1, type = "class" , newdata = pd.test)
table(nn.pred,pd.test$Loan_Status)
mean(nn.pred != pd.test.Y)
x = c(Gender)
nn.fit = multinom(Loan_Status ~ Married + Dependents + Education + Self_Employed + ApplicantIncome + CoapplicantIncome + LoanAmount+ Loan_Amount_Term + Credit_History,data = pd.train)
nn.fit = multinom(Loan_Status ~ Married + Dependents + Education + Self_Employed + ApplicantIncome + CoapplicantIncome + LoanAmount+ Loan_Amount_Term + Credit_History,data = pd.train)
summary(nn.fit)
nn.pred <- predict(nn.fit, type = "class" , newdata = pd.test)
mean(nn.pred != pd.test.Y)
table(nn.pred,pd.test$Loan_Status)
nn.fit2 = multinom(Loan_Status ~ Married + Dependents + Education + Self_Employed + ApplicantIncome + CoapplicantIncome + LoanAmount+ Loan_Amount_Term + Credit_History,data = pd.train)
summary(nn.fit2)
nn.pred <- predict(nn.fit2, type = "class" , newdata = pd.test)
table(nn.pred,pd.test$Loan_Status)
nn.fit2 = multinom(Loan_Status ~ Married + Dependents + Education + Self_Employed + ApplicantIncome + CoapplicantIncome + LoanAmount+ Loan_Amount_Term,data = pd.train)
summary(nn.fit2)
nn.pred <- predict(nn.fit2, type = "class" , newdata = pd.test)
table(nn.pred,pd.test$Loan_Status)
mean(nn.pred != pd.test.Y)
# Train with Logistic Regression
library(mlbench)
library(caret)
# Train with Logistic Regression
install.packages("mlbench")
install.packages("caret")
library(mlbench)
library(caret)
correlationMatrix <- cor(pd.train.X)
correlationMatrix <- cor(pd.train[,7:10])
print(correlationMatrix)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.5)
print(highlyCorrelated)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0)
print(highlyCorrelated)
# ApplicantIncome and LoanAmount are highly correlated(0.55)
importance <- varImp(nn.fit1, scale=FALSE)
print(importance)
plot(importance)
# ApplicantIncome and LoanAmount are highly correlated(0.55)
importance <- varImp(nn.fit1, scale=TRUE)
print(importance)
plot(importance)
rank(importance)
sort(importance)
# ApplicantIncome and LoanAmount are highly correlated(0.55)
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# ApplicantIncome and LoanAmount are highly correlated(0.55)
control <- trainControl(method="repeatedcv", number=10, repeats=3)
nn.fit1 <- multinom(Loan_Status ~ Married + Dependents + Education + Self_Employed + ApplicantIncome + CoapplicantIncome + LoanAmount+ Loan_Amount_Term + Credit_History + Property_Area,data = pd.train,preProcess = "scale",trControl = control)
importance <- varImp(nn.fit1, scale=FALSE)
plot(importance)
print(importance)
# ApplicantIncome and LoanAmount are highly correlated(0.55)
nn.fit1 <- multinom(Loan_Status ~ Married + Dependents + Education + Self_Employed + ApplicantIncome + CoapplicantIncome + LoanAmount+ Loan_Amount_Term + Credit_History + Property_Area,data = pd.train)
importance <- varImp(nn.fit1, scale=FALSE)
print(importance)
importance <- varImp(nn.fit1, scale=TRUE)
print(importance)
# Credit history is the most important
control <- rfeControl(functions=logistic, method="cv", number=10)
# Credit history is the most important
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
results <- rfe(pd.train.X, pd.test.Y, sizes=c(1:10), rfeControl=control)
results <- rfe(pd.train.X, pd.train.Y, sizes=c(1:10), rfeControl=control)
nn.fit1 <- multinom(Loan_Status ~ Married + Dependents + Education + Self_Employed + ApplicantIncome + CoapplicantIncome + LoanAmount+ Loan_Amount_Term + Credit_History + Property_Area,data = pd.train)
summary(nn.fit1)
nn.pred <- predict(nn.fit1, type = "class" , newdata = pd.test)
table(nn.pred,pd.test$Loan_Status)
mean(nn.pred != pd.test.Y)
nn.fit2 = multinom(Loan_Status ~ Married + Dependents + Education + Self_Employed + ApplicantIncome + CoapplicantIncome + Loan_Amount_Term + Credit_History + Property_Area,data = pd.train)
summary(nn.fit2)
nn.pred <- predict(nn.fit2, type = "class" , newdata = pd.test)
table(nn.pred,pd.test$Loan_Status)
mean(nn.pred != pd.test.Y)
nn.fit3 = multinom(Loan_Status ~ Married + Dependents + Education + Self_Employed + CoapplicantIncome + LoanAmount + Loan_Amount_Term + Credit_History + Property_Area,data = pd.train)
summary(nn.fit3)
nn.pred <- predict(nn.fit3, type = "class" , newdata = pd.test)
table(nn.pred,pd.test$Loan_Status)
mean(nn.pred != pd.test.Y)
nn.pred <- predict(nn.fit1, type = "class" , newdata = pd.test)
table(nn.pred,pd.test$Loan_Status)
mean(nn.pred != pd.test.Y)
nn.pred <- predict(nn.fit3, type = "class" , newdata = pd.test)
table(nn.pred,pd.test$Loan_Status)
mean(nn.pred != pd.test.Y)
nn.fit4 = multinom(Loan_Status ~ Married + Dependents + Education + Self_Employed + CoapplicantIncome + Loan_Amount_Term + Credit_History + Property_Area,data = pd.train)
summary(nn.fit4)
nn.pred <- predict(nn.fit4, type = "class" , newdata = pd.test)
table(nn.pred,pd.test$Loan_Status)
mean(nn.pred != pd.test.Y)
# ApplicantIncome and LoanAmount are highly correlated(0.55)
nn.fit1 <- multinom(Loan_Status ~ Gender + Married + Dependents + Education + Self_Employed + ApplicantIncome + CoapplicantIncome + LoanAmount+ Loan_Amount_Term + Credit_History + Property_Area,data = pd.train)
summary(nn.fit1)
nn.pred <- predict(nn.fit1, type = "class" , newdata = pd.test)
table(nn.pred,pd.test$Loan_Status)
mean(nn.pred != pd.test.Y)
nn.fit4 = multinom(Loan_Status ~ Married + Dependents + Education + Self_Employed + CoapplicantIncome + Loan_Amount_Term + Credit_History ,data = pd.train)
summary(nn.fit4)
nn.pred <- predict(nn.fit4, type = "class" , newdata = pd.test)
table(nn.pred,pd.test$Loan_Status)
mean(nn.pred != pd.test.Y)
nn.fit4 = multinom(Loan_Status ~ Credit_History ,data = pd.train)
summary(nn.fit4)
nn.pred <- predict(nn.fit4, type = "class" , newdata = pd.test)
table(nn.pred,pd.test$Loan_Status)
mean(nn.pred != pd.test.Y)
nn.fit4 = multinom(Loan_Status ~ Dependents + Education + Self_Employed + CoapplicantIncome + Loan_Amount_Term + Credit_History + Property_Area ,data = pd.train)
summary(nn.fit4)
nn.pred <- predict(nn.fit4, type = "class" , newdata = pd.test)
table(nn.pred,pd.test$Loan_Status)
mean(nn.pred != pd.test.Y)
nn.fit4 = multinom(Loan_Status ~ Married + Dependents + Education + Self_Employed + CoapplicantIncome + Loan_Amount_Term + Credit_History + Property_Area,data = pd.train)
summary(nn.fit4)
nn.pred <- predict(nn.fit4, type = "class" , newdata = pd.test)
table(nn.pred,pd.test$Loan_Status)
mean(nn.pred != pd.test.Y)
nn.fit1 <- multinom(Loan_Status ~ Married + Dependents + Education + Self_Employed + ApplicantIncome + CoapplicantIncome + LoanAmount+ Loan_Amount_Term + Credit_History + Property_Area,data = pd.train)
summary(nn.fit1)
nn.pred <- predict(nn.fit1, type = "class" , newdata = pd.test)
table(nn.pred,pd.test$Loan_Status)
mean(nn.pred != pd.test.Y)
importance <- varImp(nn.fit1, scale=FALSE)
print(importance)
plot(importance)
nn.fit1 <- multinom(Loan_Status ~ Gender + Married + Dependents + Education + Self_Employed + ApplicantIncome + CoapplicantIncome + LoanAmount+ Loan_Amount_Term + Credit_History + Property_Area,data = pd.train)
summary(nn.fit1)
nn.pred <- predict(nn.fit1, type = "class" , newdata = pd.test)
table(nn.pred,pd.test$Loan_Status)
mean(nn.pred != pd.test.Y)
nn.fit2 = multinom(Loan_Status ~ Credit_History ,data = pd.train)
summary(nn.fit2)
nn.pred <- predict(nn.fit2, type = "class" , newdata = pd.test)
table(nn.pred,pd.test$Loan_Status)
mean(nn.pred != pd.test.Y)
nn.fit1 <- multinom(Loan_Status ~ Gender + Married + Dependents + Education + Self_Employed + ApplicantIncome + CoapplicantIncome + LoanAmount+ Loan_Amount_Term + Credit_History + Property_Area,data = pd.train)
summary(nn.fit1)
nn.pred <- predict(nn.fit1, type = "class" , newdata = pd.test)
table(nn.pred,pd.test$Loan_Status)
mean(nn.pred != pd.test.Y)
importance <- varImp(nn.fit1, scale=FALSE)
print(importance)
plot(importance)
nn.fit2 = multinom(Loan_Status ~ Credit_History ,data = pd.train)
summary(nn.fit2)
nn.pred <- predict(nn.fit2, type = "class" , newdata = pd.test)
table(nn.pred,pd.test$Loan_Status)
mean(nn.pred != pd.test.Y)
#Train with SVM
library(e1071)
set.seed(1)
tune.out <- tune(svm,Loan_Status~.,
data=pd.train,
kernel="linear",
ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)),tunecontrol=tune.control(cross=5))
source("~/GitHub/MH6811-Machine-Learning-Project/MH6811 Group Project G10.R", echo=TRUE)
library(mlbench)
library(caret)
correlationMatrix <- cor(pd.train[,7:10])
print(correlationMatrix)
library(nnet)
nn.fit1 <- multinom(Loan_Status ~ Gender + Married + Dependents + Education + Self_Employed + ApplicantIncome + CoapplicantIncome + LoanAmount+ Loan_Amount_Term + Credit_History + Property_Area,data = pd.train)
summary(nn.fit1)
nn.pred <- predict(nn.fit1, type = "class" , newdata = pd.test)
table(nn.pred,pd.test$Loan_Status)
mean(nn.pred != pd.test.Y)
importance <- varImp(nn.fit1, scale=FALSE)
print(importance)
plot(importance)
nn.fit2 = multinom(Loan_Status ~ Credit_History ,data = pd.train)
summary(nn.fit2)
nn.pred <- predict(nn.fit2, type = "class" , newdata = pd.test)
table(nn.pred,pd.test$Loan_Status)
mean(nn.pred != pd.test.Y)
#Train with SVM
library(e1071)
set.seed(1)
tune.out <- tune(svm,Loan_Status~.,
data=pd.train,
kernel="linear",
ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)),tunecontrol=tune.control(cross=5))
set.seed(1)
>>>>>>> Stashed changes
library(CatEncoders)
t <- function(x) {
# check if x is numeric
if(is.numeric(x)) {
return (x)
}
l <- LabelEncoder.fit(x)
y <- transform(l, x)
return (y)
}
<<<<<<< Updated upstream
ffnn.train.X <- sapply(pd.train.X, t)
ffnn.test.X <- sapply(pd.test.X, t)
ffnn.train.X.mean <- apply(ffnn.train.X, 2, mean)
ffnn.train.X.sd <- apply(ffnn.train.X, 2, sd)
ffnn.train.X.scaled<-scale(ffnn.train.X,center=ffnn.train.X.mean,scale=ffnn.train.X.sd)
ffnn.test.X.scaled<-scale(ffnn.test.X,center=ffnn.train.X.mean,scale=ffnn.train.X.sd)
ffnn.train.scaled<-data.frame(ffnn.train.X.scaled,Loan_Status=pd.train.Y)
ffnn.test.scaled<-data.frame(ffnn.test.X.scaled,Loan_Status=pd.test.Y)
#ffnn.train.X.scaled<-as.matrix(ffnn.train.X.scaled)
#ffnn.test.X.scaled<-as.matrix(ffnn.test.X.scaled)
set.seed(114514)
# create sequential model
ffnn.keras.model = keras_model_sequential()
# add layers, first layer needs input dimension
ffnn.keras.model %>%
layer_dense(input_shape = ncol(ffnn.train.X.scaled), units = 10, activation = "relu") %>%
layer_dense(units = 10, activation = "relu") %>%
layer_dense(units = 3, activation = "softmax")
# add a loss function and optimizer
ffnn.keras.model %>%
compile(
loss = "categorical_crossentropy",
optimizer = "rmsprop",
metrics = "accuracy"
)
# fit model with our training data set, training will be done for 200 times data set
loan.ffnn.keras = ffnn.keras.model %>%
fit(
x = ffnn.train.X.scaled,
y = y.keras.ohc,
shuffle = T,
epochs = 30
)
# Standardization of the features:
=======
svm.train.X <- sapply(pd.train.X, t)
svm.test.X <- sapply(pd.test.X, t)
svm.train <- sapply(pd.train, t)
svm.test <- sapply(pd.test, t)
tune.out <- tune(svm,Loan_Status~.,
data=svm.train,
kernel="linear",
ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)),tunecontrol=tune.control(cross=5))
svm.train = data.frame(svm.train)
svm.test = data.frame(svm.test)
tune.out <- tune(svm,Loan_Status~.,
data=svm.train,
kernel="linear",
ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)),tunecontrol=tune.control(cross=5))
summary(tune,out)
summary(tune.out)
svm.best <- tune.out$best.model
summary(svm.best)
svm.pred <- predict(svm.best, svm.test)
table(svm.pred, svm.test$Loan_Status)
svm.pred  = round(svm.pred)
table(svm.pred, svm.test$Loan_Status)
mean(svm.pred != svm.test$Loan_Status )
#Polynomial Kernel
set.seed(1)
tune.out <- tune(svm,Loan_Status~.,
data=svm.train,
kernel="polynomial",
ranges=list(cost=c(0.001,0.01,0.1,1),
degree=c(2,3,4,5)))
summary(tune.out)
svm.best <- tune.out$best.model
summary(svm.best)
svm.pred <- predict(svm.best, svm.test)
svm.pred  = round(svm.pred)
table(svm.pred, svm.test$Loan_Status)
mean(svm.pred != svm.test$Loan_Status )
help("round")
help("round")
help("round")
for x in svm.pred{
if x<1:
x = 1
}
for x in 0:len(svm.pred){
if svm.pred[x]<1:
svm.pred[x] = 1
}
svm.pred
svm.pred[1]
svm.pred[1].values
svm.pred[1].value
svm.pred[1]
svm.pred[1] = 1
svm.pred[1] == 1
svm.pred[1] < 1
svm.pred[180] < 1
for x in 1:len(svm.pred){
if svm.pred[x]<1:
svm.pred[x] = 1
}
if svm.pred[x]<1:
svm.pred[x] = 1
svm.pred[x] = 1
svm.pred[180] < 1
for(x in 1:len(svm.pred)) {
if (svm.pred[x]<1){
svm.pred[x] = 1
}
}
len(svm.pred)
length(svm.pred)
for(x in 1:length(svm.pred)) {
if (svm.pred[x]<1){
svm.pred[x] = 1
}
}
table(svm.pred, svm.test$Loan_Status)
mean(svm.pred != svm.test$Loan_Status )
#RBF Kernel
set.seed(1)
tune.out <- tune(svm,Loan_Status~.,
data=svm.train,
kernel="radial",
ranges=list(cost=c(0.001,0.01,0.1,1),
gamma=c(0.01,0.1,1,10)))
summary(tune.out)
svm.best <- tune.out$best.model
summary(svm.best)
svm.pred <- predict(svm.best, svm.test)
svm.pred  = round(svm.pred)
for(x in 1:length(svm.pred)) {
if (svm.pred[x]<1){
svm.pred[x] = 1
}
}
table(svm.pred, svm.test$Loan_Status)
mean(svm.pred != svm.test$Loan_Status )
importance <- varImp(svm.best, scale=FALSE)
#Linear Kernel
library(mlbench)
library(caret)
importance <- varImp(svm.best, scale=FALSE)
#Linear Kernel
library(mlbench)
library(caret)
install.packages("caret")
library(caret)
library(caret)
install.packages("caret")
library(caret)
install.packages("caret",dependencies = TRUE)
library(caret)
#Linear Kernel
library(mlbench)
library(caret)
tune.out <- tune(svm,Loan_Status~.,
data=svm.train,
kernel="linear",
ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)),tunecontrol=tune.control(cross=5))
summary(tune.out)
#Train with SVM
library(e1071)
set.seed(1)
>>>>>>> Stashed changes
library(CatEncoders)
t <- function(x) {
# check if x is numeric
if(is.numeric(x)) {
return (x)
}
l <- LabelEncoder.fit(x)
y <- transform(l, x)
return (y)
}
<<<<<<< Updated upstream
ffnn.train.X <- sapply(pd.train.X, t)
ffnn.test.X <- sapply(pd.test.X, t)
ffnn.train.X.mean <- apply(ffnn.train.X, 2, mean)
ffnn.train.X.sd <- apply(ffnn.train.X, 2, sd)
ffnn.train.X.scaled<-scale(ffnn.train.X,center=ffnn.train.X.mean,scale=ffnn.train.X.sd)
ffnn.test.X.scaled<-scale(ffnn.test.X,center=ffnn.train.X.mean,scale=ffnn.train.X.sd)
ffnn.train.scaled<-data.frame(ffnn.train.X.scaled,Loan_Status=pd.train.Y)
ffnn.test.scaled<-data.frame(ffnn.test.X.scaled,Loan_Status=pd.test.Y)
ffnn.train.X.scaled<-as.matrix(ffnn.train.X.scaled)
ffnn.test.X.scaled<-as.matrix(ffnn.test.X.scaled)
str(ffnn.train.X.scaled)
wtf <- as.matrix(ffnn.train.X.scaled)
View(wtf)
set.seed(114514)
# create sequential model
ffnn.keras.model = keras_model_sequential()
# add layers, first layer needs input dimension
ffnn.keras.model %>%
layer_dense(input_shape = ncol(ffnn.train.X.scaled), units = 10, activation = "relu") %>%
layer_dense(units = 10, activation = "relu") %>%
layer_dense(units = 3, activation = "softmax")
# add a loss function and optimizer
ffnn.keras.model %>%
compile(
loss = "categorical_crossentropy",
optimizer = "rmsprop",
metrics = "accuracy"
)
# fit model with our training data set, training will be done for 200 times data set
loan.ffnn.keras = ffnn.keras.model %>%
fit(
x = wtf,
y = y.keras.ohc,
shuffle = T,
epochs = 30
)
View(y.keras.ohc)
# Feedforward Neural Networks with Keras and TensorFlow
library(keras)
library(tensorflow)
y.keras <- as.factor(pd.train.Y)
levels(y.keras) = 0:1
y.keras.ohc = to_categorical(y.keras , num_classes = 2)
# Feedforward Neural Networks with Keras and TensorFlow
library(keras)
library(tensorflow)
y.keras <- as.factor(pd.train.Y)
y.keras.ohc = to_categorical(y.keras , num_classes = 2)
# Feedforward Neural Networks with Keras and TensorFlow
library(keras)
library(tensorflow)
y.keras <- as.factor(pd.train.Y)
levels(y.keras) = 0:1
y.keras.ohc = to_categorical(y.keras , num_classes = 2)
View(y.keras.ohc)
set.seed(114514)
# create sequential model
ffnn.keras.model = keras_model_sequential()
# add layers, first layer needs input dimension
ffnn.keras.model %>%
layer_dense(input_shape = ncol(ffnn.train.X.scaled), units = 10, activation = "relu") %>%
layer_dense(units = 10, activation = "relu") %>%
layer_dense(units = 3, activation = "softmax")
# add a loss function and optimizer
ffnn.keras.model %>%
compile(
loss = "categorical_crossentropy",
optimizer = "rmsprop",
metrics = "accuracy"
)
# fit model with our training data set, training will be done for 200 times data set
loan.ffnn.keras = ffnn.keras.model %>%
fit(
x = wtf,
y = y.keras.ohc,
shuffle = T,
epochs = 30
)
set.seed(114514)
# create sequential model
ffnn.keras.model = keras_model_sequential()
# add layers, first layer needs input dimension
ffnn.keras.model %>%
layer_dense(input_shape = ncol(ffnn.train.X.scaled), units = 10, activation = "relu") %>%
layer_dense(units = 10, activation = "relu") %>%
layer_dense(units = 3, activation = "softmax")
# add a loss function and optimizer
ffnn.keras.model %>%
compile(
loss = "categorical_crossentropy",
optimizer = "rmsprop",
metrics = "accuracy"
)
# fit model with our training data set, training will be done for 200 times data set
loan.ffnn.keras = ffnn.keras.model %>%
fit(
x = ffnn.train.X.scaled,
y = y.keras.ohc,
shuffle = T,
epochs = 30
)
ncol(ffnn.train.X.scaled)
set.seed(114514)
# create sequential model
ffnn.keras.model = keras_model_sequential()
# add layers, first layer needs input dimension
ffnn.keras.model %>%
layer_dense(input_shape = ncol(ffnn.train.X.scaled), units = 10, activation = "relu") %>%
layer_dense(units = 10, activation = "relu") %>%
layer_dense(units = 3, activation = "softmax")
# add a loss function and optimizer
ffnn.keras.model %>%
compile(
loss = "categorical_crossentropy",
optimizer = "rmsprop",
metrics = "accuracy"
)
# fit model with our training data set, training will be done for 200 times data set
loan.ffnn.keras = ffnn.keras.model %>%
fit(
x = ffnn.train.X.scaled,
y = y.keras.ohc,
shuffle = T,
epochs = 30
)
set.seed(114514)
# create sequential model
ffnn.keras.model = keras_model_sequential()
# add layers, first layer needs input dimension
ffnn.keras.model %>%
layer_dense(input_shape = ncol(ffnn.train.X.scaled), units = 10, activation = "relu") %>%
layer_dense(units = 10, activation = "relu") %>%
layer_dense(units = 3, activation = "softmax")
# add a loss function and optimizer
ffnn.keras.model %>%
compile(
loss = "categorical_crossentropy",
optimizer = "rmsprop",
metrics = "accuracy"
)
# fit model with our training data set, training will be done for 200 times data set
loan.ffnn.keras = ffnn.keras.model %>%
fit(
x = ffnn.train.X,
y = y.keras.ohc,
shuffle = T,
epochs = 30
)
set.seed(114514)
# create sequential model
ffnn.keras.model = keras_model_sequential()
# add layers, first layer needs input dimension
ffnn.keras.model %>%
layer_dense(input_shape = ncol(ffnn.train.X.scaled), units = 10, activation = "relu") %>%
layer_dense(units = 10, activation = "relu") %>%
layer_dense(units = 3, activation = "softmax")
# add a loss function and optimizer
ffnn.keras.model %>%
compile(
loss = "categorical_crossentropy",
optimizer = "rmsprop",
metrics = "accuracy"
)
# fit model with our training data set, training will be done for 200 times data set
loan.ffnn.keras = ffnn.keras.model %>%
fit(
x = pd.train.X,
y = y.keras.ohc,
shuffle = T,
epochs = 30
)
# Feedforward Neural Networks with Keras and TensorFlow
library(keras)
library(tensorflow)
y.keras <- as.factor(pd.train.Y)
levels(y.keras) = 1:2
y.keras.ohc = to_categorical(y.keras , num_classes = 2)
# Feedforward Neural Networks with Keras and TensorFlow
library(keras)
library(tensorflow)
y.keras <- as.factor(pd.train.Y)
levels(y.keras) = c("N","Y")
y.keras.ohc = to_categorical(y.keras , num_classes = 2)
# Feedforward Neural Networks with Keras and TensorFlow
library(keras)
library(tensorflow)
y.keras <- as.factor(pd.train.Y)
levels(y.keras) = 0:1
y.keras.ohc = to_categorical(y.keras , num_classes = 2)
library(keras)
library(tensorflow)
data(iris)
library(keras)
library(tensorflow)
data(iris)
# summary(iris)
set.seed(1)
train <- sample(nrow(iris), 0.7 * nrow(iris))
X.train <- subset(iris[train, ],select=-Species)
y.train <- iris$Species[train]
X.test <- subset(iris[-train, ],select=-Species)
y.test <- iris$Species[-train]
X.train.mean <- apply(X.train, 2, mean)
X.train.sd <- apply(X.train, 2, sd)
X.train.scaled<-scale(X.train,center=X.train.mean,scale=X.train.sd)
X.test.scaled<-scale(X.test,center=X.train.mean,scale=X.train.sd)
# iris.train<-data.frame(X.train,Species=y.train)
# iris.test<-data.frame(X.test,Species=y.test)
iris.train.scaled<-data.frame(X.train.scaled,Species=y.train)
iris.test.scaled<-data.frame(X.test.scaled,Species=y.test)
# X.train<-as.matrix(X.train)
X.train.scaled<-as.matrix(X.train.scaled)
# X.test<-as.matrix(X.test)
X.test.scaled<-as.matrix(X.test.scaled)
y <- y.train
levels(y) = 0:2
y.train.ohc = to_categorical(y , num_classes = 3)
View(X.train.scaled)
set.seed(114514)
# create sequential model
ffnn.keras.model = keras_model_sequential()
# add layers, first layer needs input dimension
ffnn.keras.model %>%
layer_dense(input_shape = ncol(ffnn.train.X.scaled), units = 10, activation = "relu") %>%
layer_dense(units = 10, activation = "relu") %>%
layer_dense(units = 2, activation = "softmax")
# add a loss function and optimizer
ffnn.keras.model %>%
compile(
loss = "categorical_crossentropy",
optimizer = "rmsprop",
metrics = "accuracy"
)
# fit model with our training data set, training will be done for 200 times data set
loan.ffnn.keras = ffnn.keras.model %>%
fit(
x = ffnn.train.X.scaled,
y = y.keras.ohc,
shuffle = T,
epochs = 30
)
set.seed(114514)
# create sequential model
ffnn.keras.model = keras_model_sequential()
# add layers, first layer needs input dimension
ffnn.keras.model %>%
layer_dense(input_shape = ncol(ffnn.train.X.scaled), units = 10, activation = "relu") %>%
layer_dense(units = 10, activation = "relu") %>%
layer_dense(units = 2, activation = "softmax")
# add a loss function and optimizer
ffnn.keras.model %>%
compile(
loss = "categorical_crossentropy",
optimizer = "rmsprop",
metrics = "accuracy"
)
# fit model with our training data set, training will be done for 200 times data set
loan.ffnn.keras = ffnn.keras.model %>%
fit(
x = ffnn.train.X.scaled,
y = y.keras.ohc,
shuffle = T,
epochs = 100
)
y <- pd.test.Y
levels(y) = 0:1
y.keras.test.ohc = to_categorical(y , num_classes = 2)
y <- as.factor(pd.test.Y)
levels(y) = 0:1
y.keras.test.ohc = to_categorical(y , num_classes = 2)
evaluate(model, ffnn.test.X.scaled, y.keras.test.ohc, batch_size =1)
y <- as.factor(pd.test.Y)
levels(y) = 0:1
y.keras.test.ohc = to_categorical(y , num_classes = 2)
evaluate(loan.ffnn.keras, ffnn.test.X.scaled, y.keras.test.ohc, batch_size =1)
y <- as.factor(pd.test.Y)
levels(y) = 0:1
y.keras.test.ohc = to_categorical(y , num_classes = 2)
evaluate(ffnn.keras.model, ffnn.test.X.scaled, y.keras.test.ohc, batch_size =1)
y.keras <- as.factor(pd.test.Y)
levels(y.keras) = 0:1
y.keras.test.ohc = to_categorical(y , num_classes = 2)
evaluate(ffnn.keras.model, ffnn.test.X.scaled, y.keras.test.ohc, batch_size =1)
loan.keras.pred.prob<-predict(ffnn.keras.model, ffnn.test.X.scaled, y.keras.test.ohc, batch_size =1)
loan.keras.pred<-as.factor(max.col(loan.keras.pred.prob))
levels(loan.keras.pred)=y.keras.levelnames
loan.keras.pred.prob<-predict(ffnn.keras.model, ffnn.test.X.scaled, y.keras.test.ohc, batch_size =1)
loan.keras.pred<-as.factor(max.col(loan.keras.pred.prob))
levels(loan.keras.pred) <- levels(pd.test.Y)
loan.keras.pred.prob<-predict(ffnn.keras.model, ffnn.test.X.scaled, y.keras.test.ohc, batch_size =1)
loan.keras.pred<-as.factor(max.col(loan.keras.pred.prob))
levels(loan.keras.pred) <- levels(as.factor(pd.test.Y))
table(loan.keras.pred, pd.test.Y)
loan.keras.pred.prob<-predict(ffnn.keras.model, ffnn.test.X.scaled, y.keras.test.ohc, batch_size =1)
loan.keras.pred<-as.factor(max.col(loan.keras.pred.prob))
levels(loan.keras.pred) <- levels(as.factor(pd.test.Y))
keras.pred.table <- table(loan.keras.pred, pd.test.Y)
confusionMatrix(keras.pred.table)
=======
svm.train <- sapply(pd.train, t)
svm.test <- sapply(pd.test, t)
svm.train = data.frame(svm.train)
svm.test = data.frame(svm.test)
#Linear Kernel
library(mlbench)
library(caret)
tune.out <- tune(svm,Loan_Status~.,
data=svm.train,
kernel="linear",
ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)),tunecontrol=tune.control(cross=5))
summary(tune.out)
svm.best <- tune.out$best.model
summary(svm.best)
svm.pred <- predict(svm.best, svm.test)
svm.pred  = round(svm.pred)
table(svm.pred, svm.test$Loan_Status)
mean(svm.pred != svm.test$Loan_Status )
importance <- varImp(svm.best, scale=FALSE)
print(importance)
importance <- varImp(svm.best, scale=FALSE)
library(caret)
library(mlbench)
library(caret)
correlationMatrix <- cor(pd.train[,7:10])
print(correlationMatrix)
library(nnet)
nn.fit1 <- multinom(Loan_Status ~ Gender + Married + Dependents + Education + Self_Employed + ApplicantIncome + CoapplicantIncome + LoanAmount+ Loan_Amount_Term + Credit_History + Property_Area,data = pd.train)
summary(nn.fit1)
nn.pred <- predict(nn.fit1, type = "class" , newdata = pd.test)
table(nn.pred,pd.test$Loan_Status)
mean(nn.pred != pd.test.Y)
importance <- varImp(nn.fit1, scale=FALSE)
print(importance)
plot(importance)
importance <- varImp(nn.fit1, scale=FALSE)
importance <- varImp(svm.best, scale=FALSE)
print(importance)
plot(importance)
#Polynomial Kernel
set.seed(1)
tune.out <- tune(svm,Loan_Status~.,
data=svm.train,
kernel="polynomial",
ranges=list(cost=c(0.001,0.01,0.1,1),
degree=c(2,3,4,5)))
summary(tune.out)
svm.best <- tune.out$best.model
summary(svm.best)
svm.pred <- predict(svm.best, svm.test)
svm.pred  = round(svm.pred)
for(x in 1:length(svm.pred)) {
if (svm.pred[x]<1){
svm.pred[x] = 1
}
}
table(svm.pred, svm.test$Loan_Status)
mean(svm.pred != svm.test$Loan_Status )
#RBF Kernel
set.seed(1)
tune.out <- tune(svm,Loan_Status~.,
data=svm.train,
kernel="radial",
ranges=list(cost=c(0.001,0.01,0.1,1),
gamma=c(0.01,0.1,1,10)))
summary(tune.out)
svm.best <- tune.out$best.model
summary(svm.best)
svm.pred <- predict(svm.best, svm.test)
svm.pred  = round(svm.pred)
for(x in 1:length(svm.pred)) {
if (svm.pred[x]<1){
svm.pred[x] = 1
}
}
table(svm.pred, svm.test$Loan_Status)
mean(svm.pred != svm.test$Loan_Status )
library(mlbench)
library(caret)
install.packages("e1071")
library(mlbench)
library(caret)
install.packages("caret",
repos = "http://cran.r-project.org",
dependencies = c("Depends", "Imports", "Suggests"))
library(mlbench)
library(caret)
#Linear Kernel
library(mlbench)
library(caret)
>>>>>>> Stashed changes
